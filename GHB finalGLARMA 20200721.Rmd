---
title: "GHB final 2020"
author: "Sulyok"
date: "July 19, 2020"
output: word_document
---

```{r echo=TRUE}
setwd("~/Rprojects/GHB")
Sys.setenv(LANG = "en")
sessionInfo()
library(haven)
data <- read_sav("GHB_tox_adat_140711.sav")
GTD <- read_sav("GHB_tox_google.sav")
#GTD<-GTD[,-c(6:9)]
#GTD<-na.omit(GTD)
GTD<-GTD[-47,]
counts<-table(data$FELVETEL)
counts<-as.data.frame(counts)


GTD$date <- seq(as.Date("2009-09-14"), as.Date("2013-07-13"), "month" )

library(ggplot2)


p<-ggplot(GTD, aes(x = GTD$date)) +
geom_point(aes(y=GTD$tox_eset*5, colour="Admission numbers"), alpha= 0.4) + geom_smooth(aes(y=GTD$tox_eset*5, colour="Admission numbers"), span = 0.2, se=FALSE) +
geom_point(aes(y=GTD$google1, col="GHB-related seach volumes"), alpha= 0.4) + geom_smooth(aes(y=GTD$google1, col="GHB-related seach volumes"), span = 0.3, se=FALSE) +
 geom_point(aes(y=GTD$google2, col="Gina-related seach volumes"), alpha= 0.4) + geom_smooth(aes(y=GTD$google2, col="Gina-related seach volumes"), span = 0.3, se=FALSE) + 
  scale_colour_manual("", breaks = c("Admission numbers", "GHB-related seach volumes","Gina-related seach volumes") , values = c("red", "blue", "green")) +
scale_x_date(date_minor_breaks = "1 month") +
xlab(NULL) +
ylab("Google search volume") + ylim(c(0, 100)) +
  scale_y_continuous(sec.axis = sec_axis(~./5, name = "Monthly admission number")) +
theme_bw() +  theme(legend.position = "none") 

p


cortesttoxeset<-function(x){cor.test(x, GTD$tox_eset, method="kendall")}
contempcorrsmonthtox_eset<-lapply( GTD[, 4:9], cortesttoxeset)
contempcorrsmonthtox_eset



library(lubridate)
counts$Var1 <- ymd(counts$Var1)

# extract week year

week <- function(x)format(x, '%Y.%W')

counts$weekyear<-week(counts$Var1)

month<- function(x)format(x, '%Y.%B')

counts$month<-month(counts$Var1)
#create a df with 0-counts for all days

n<-rep(0, 1369)
n<-as.data.frame(n)
n$Var1<-seq(as.Date("2009-09-14"), as.Date("2013-06-13"), "day" )

#join them
library(dplyr)
counts<-full_join(n, counts, by="Var1")
counts$Freq[is.na(counts$Freq)] <- 0 

# extract week year

week <- function(x)format(x, '%Y.%W')

counts$weekyear<-week(counts$Var1)
summary(counts)

counts$month<-month(counts$Var1)



# extract week year

GTD$weekyear<-week(GTD$date)
GTD$month<-month(GTD$date)

#join

library(dplyr)
counts1<-full_join(counts, GTD, by="month")
hist(counts1$Freq)
mean(counts1$Freq)
var(counts1$Freq)

counts$date<-NULL
counts1$n<-NULL
summary(counts1)
ts <- ts(counts1$Freq, start=c(2009, 9, 14), end=c(2013, 6, 14), frequency=365) 
counts1$weekday<-weekdays(counts1$Var1)


kruskal.test(counts1$Freq, as.factor(counts1$weekday))
library(dunn.test)
dunn.test(counts1$Freq, as.factor(counts1$weekday), method="holm")

#crosscorr months

ccfKendall1<- sapply( -12:12, function(l) cor.test( GTD$tox_eset, Hmisc::Lag(GTD$google1,l),method =
"kendall", use ="complete.obs")$estimate )
ccfKendall1


plot(-12:12,ccfKendall1,type="h")
abline(h=0)

ccfKendall2<- sapply( -12:12, function(l) cor.test( GTD$tox_eset, Hmisc::Lag(GTD$google2,l),method =
"kendall", use ="complete.obs")$estimate )
ccfKendall2

plot(-12:12,ccfKendall2,type="h")
abline(h=0)


library(glarma)



##monthly data
setwd("~/Rprojects/GHB")
library(haven)
data <- read_sav("GHB_tox_adat_140711.sav")
GTD <- read_sav("GHB_tox_google.sav")
GTD<-GTD[-47,]
library(forecast)
ts <- ts(GTD$tox_eset, start=c(2009, 9, 14), end=c(2013, 6, 14), frequency=12) 
monthplot(ts)
seasonplot(ts)
decomp<-decompose(ts)
plot(decomp)
monthplot(decomp, choice="seasonal")
autoplot(ts)+ geom_smooth(method="loess") + ylab("GHB-related admission rates")
ggseasonplot(ts, year.labels=TRUE, year.labels.left=TRUE) +
  ylab("Admission numbers") +
  ggtitle("Seasonal plot: GHB-related hospital admissions")
ggseasonplot(ts, year.labels=TRUE, year.labels.left=TRUE, polar=TRUE) +
  ylab("Admission numbers") +
  ggtitle("Seasonal plot: GHB-related hospital admissions")

tsg1 <- ts(GTD$google1, start=c(2009, 9, 14), end=c(2013, 6, 14), frequency=12) 
decomp<-decompose(tsg1)
plot(decomp)
tsg2 <- ts(GTD$google2, start=c(2009, 9, 14), end=c(2013, 6, 14), frequency=12) 
decomp<-decompose(tsg2)
plot(decomp)




library(MASS)
fit<- glm.nb(tox_eset ~ google1 + google2, data=GTD)
summary(fit)
plot(fit)
estim<-cbind(fit$coefficients, confint(fit))
estim
exp(estim)








GTD$predicted<-predict(fit)
GTD$predictedcounts<-exp(GTD$predicted)
library(ggplot2)
p<-ggplot(GTD, aes(google1, predictedcounts)) + geom_smooth(method="loess")  + xlab("GHB-related search volume") + ylab("Predicted monthly hospital admission ") + theme_bw()
p




m2 <- glm(tox_eset ~  google1 + google2, data=GTD, family = "poisson")
pchisq(2 * (logLik(fit) - logLik(m2)), df = 1, lower.tail = FALSE)
library(pscl) #ceck zero inflation 
mod2 <- zeroinfl(tox_eset ~  google1 + google2, data=GTD, dist="negbin")
summary(mod2)
aic<-AIC(fit, mod2, m2) #zeoinf is slightly better than poisson, but it is not justified (no alterative known route to count 0 generation)- negbin performs the best
aic

mean(GTD$tox_eset)
var(GTD$tox_eset)

GTD<-as.data.frame(GTD)
y<-as.vector(GTD[,2])
x<-as.matrix(GTD[,c(4,5)])
acf(residuals(fit))

glarma<-glarma(y,x, type="NegBin") #I cannot add both phi and theta lags at the same time- this has the lowest aic
glarma
summary(glarma)
plot(glarma)
#adding differnt MA and AR orders cannot wont improve aic. LRT and WALd indicates a GLM not a glarma process
mySolve2 <- function (A) {
    ErrCode <- 0
    Ainv <- try(solve(A), TRUE)
    if (is.character(Ainv)) {
        ErrCode <- 1
        Ainv <- A
    }
    list(Ainv = Ainv, ErrCode = ErrCode)
}
assignInNamespace("mySolve",mySolve2,"glarma")


glarma2<-glarma(y,x, phiLags=c(1), type="NegBin")
summary(glarma2)
AIC(glarma2)
glarma2<-glarma(y,x, phiLags=c(1,2), type="NegBin")
summary(glarma2)
glarma2<-glarma(y,x, phiLags=c(1,2,3), type="NegBin")
summary(glarma2)
glarma2<-glarma(y,x, thetaLags=c(1), type="NegBin")
summary(glarma2)
glarma2<-glarma(y,x, thetaLags=c(3), type="NegBin")
summary(glarma2)




```
